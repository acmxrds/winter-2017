{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Hate Speech in Social Media\n",
    "### By Alexandra Schofield and Thomas Davidson\n",
    "\n",
    "Hate speech refers to statements made specifically to attack or delegitimize particular groups of people based on a demographic category—race, gender, religion, sexual orientation, and so on. Below is a tutorial showcasing a few methods one can use to classify hate speech.\n",
    "\n",
    "These methods are by no means comperehensive; the lines between hate speech and other offensive language can be blurry and contextual,and existing methods do not always distill this context well. When detecting hate speech, it is important to recognize how audience, author, and context affect the intent of text and its status as hate speech. Failing to do so may result in the censorship of targeted groups as they reclaim the language used to disparage them.\n",
    "\n",
    "<span style=\"color:red\">**Content warning**</span>: the code snippet below includes slurs used as part of a lexicon to detect these terms in the document. We elected to partially censor these terms in the ACM XRDS column for which this notebook was created; here, we use a similar censoring scheme to replace vowels. While we acknowledge this does not make the words unrecognizable and still may upset those targeted by them, we hope this can mitigate their use in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Built-in Python libraries\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# Python libraries that may need to be installed. To install any of these\n",
    "# with a standard Python installation, you can run\n",
    "#   pip install <package>\n",
    "# or if you are using Anaconda to manage your Python installation,\n",
    "#   conda install <package>\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "We import tweets labeled as hate speech, offensive, or neither from a CSV. We extract the class labels and the raw text of the tweet.\n",
    "\n",
    "You can find this data on GitHub [here](https://github.com/t-davidson/hate-speech-and-offensive-language). If you use this data, please cite:\n",
    "\n",
    "Davidson, Thomas, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.\n",
    "“Automated Hate Speech Detection and the Problem of Offensive Language.” Proceedings\n",
    "of the Eleventh AAAI International Conference on Web and Social Media\n",
    "(ICWSM): 512-515."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_url = 'https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv'\n",
    "data = pandas.read_csv(data_url)\n",
    "tweets = data['tweet']\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a lexicon\n",
    "We use an example lexicon of words that are associated with sexism, racism, and homophobia. Note that we include several forms of each of these words. In a social media setting, it is likely that you might have to include many alternate spellings and misspellings to be comprehensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def substitute_char_list(str_list, list_from, list_to):\n",
    "    for cf, ct in zip(list_from, list_to):\n",
    "        str_list = [sl.replace(cf, ct) for sl in str_list]\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_orig = 'aeiouy'\n",
    "list_censored = '#$%&*!'\n",
    "hate_lexicon = [\n",
    "    'b%tch',\n",
    "    'b%tch$s',\n",
    "    'wh&r$',\n",
    "    'wh&r$s',\n",
    "    'n%gg$r',\n",
    "    'n%gg$rs',\n",
    "    'f#g',\n",
    "    'f#gs',\n",
    "    'f#gg&t',\n",
    "    'f#gg&ts']\n",
    "hate_lexicon = substitute_char_list(hate_lexicon, list_censored, list_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xanda\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "tokens = [re.split(\"[^a-zA-Z]*\", tweet.lower()) for tweet in tweets]\n",
    "X_lexicon = np.zeros((len(tweets), len(hate_lexicon) + 1))\n",
    "for i, tweet in enumerate(tweets):\n",
    "    for j, term in enumerate(hate_lexicon):\n",
    "        X_lexicon[i,j] = tweet.count(term)\n",
    "X_lexicon[:,-1] = X_lexicon.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_classifier_results(y_actual, y_pred):\n",
    "    # Obtain the confusion matrix to describe the types of classifier errors\n",
    "    conf_mat = confusion_matrix(y_actual, y_pred)\n",
    "    \n",
    "    # Some constants for computation later\n",
    "    n_classes = len(conf_mat) # should be 3\n",
    "    n_total = conf_mat.sum()\n",
    "    n_by_actual_class = conf_mat.sum(axis=1)\n",
    "    \n",
    "    print('Accuracy score: {:.2f}%\\n'.format(100 * conf_mat.trace() / n_total))\n",
    "    print('Recall by class:')\n",
    "    for cls in range(n_classes):\n",
    "        print('  {} - {:.2f}%'.format(cls, 100 * conf_mat[cls, cls] / n_by_actual_class[cls]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon experiments\n",
    "We test two versions of this classifier. In the first, we weight all examples equally, which results in the classifier naively classifying almost all examples as class 1 due to the proportions of the corpus. In the second, we force the classifier to balance the importance of precision and recall on the three classes, reducing the accuracy but improving the recall on the other two classes. In both, we use an L2 penalty (subtracting out the sum of the squares of the weights), which encouraging the combined weight of the features to be low. It's also possible to use an L1 penalty (subtracting out the sum of the absolute values of the weights), which encourages the classifier to concentrate weight in relatively few features A low value of `C` corresponds to more importance placed on the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 77.44%\n",
      "\n",
      "Recall by class:\n",
      "  0 - 0.91%\n",
      "  1 - 99.94%\n",
      "  2 - 0.00%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=\"l2\", C=0.01)\n",
    "y_pred = cross_val_predict(model, X_lexicon, y, cv=10)\n",
    "show_classifier_results(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 60.82%\n",
      "\n",
      "Recall by class:\n",
      "  0 - 34.69%\n",
      "  1 - 54.36%\n",
      "  2 - 99.59%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight=\"balanced\", penalty=\"l2\", C=0.01)\n",
    "y_pred = cross_val_predict(model, X_lexicon, y, cv=10)\n",
    "show_classifier_results(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a bag-of-words model\n",
    "\n",
    "In order to improve our classifier's accuracy at distinguishing hate speech from other offensive speech, we expand out to a much larger set of features: counts of each word's frequency in the vocabulary of the whole corpus.\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "To train a text classifier, one must *tokenize* the text, or split it into individual words or substrings in order to provide units for the classifier to process. With a relatively small supply of social media data, it is unlikely that many words will show up often enough to produce useful signals. To handle this, we do some *pre-processing* to ensure that the forms of words are more standardized.\n",
    "\n",
    "This process uses NLTK to perform stopword removal and stemming, which may require the download of specific stopword and data through the NLTK download utility. This should be installable in the *Corpora* menu under the name `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If either of these are not correctly constructed, you will\n",
    "# need to download the stopword and stemmer files using the NLTK\n",
    "# download utility and then rerun this cell. \n",
    "try:\n",
    "    stoplist = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    import nltk\n",
    "    nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from code used in https://github.com/t-davidson/hate-speech-and-offensive-language, written by Tom Davidson\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text.lower()\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split() if t not in stoplist]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "We use a `TfidfVectorizer` to load our data into a scipy sparse matrix representation, where every row corresponds to a document and every column corresponds to a word in the vocabulary. Here, the arguments mean\n",
    "* `tokenizer=tokenize`: we tokenize using the `tokenize` function to split words,\n",
    "* `preprocessor=preprocess`: we preprocess the text before tokenization using `preprocess`,\n",
    "* `use_idf=False`: we are not normalizing by the inverse document frequency (IDF) and are instead just using term frequency (TF) for each entry,\n",
    "* `decode_error='replace'`: we are replacing characters we can't convert into Unicode with a special \"replace\" character in Unicode,\n",
    "* `min_df=5`: we only keep words showing up in at least 5 tweets, and\n",
    "* `max_df=0.5`: we only keep words showing up in less than half the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xanda\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    use_idf=False,\n",
    "    decode_error='replace',\n",
    "    min_df=5,\n",
    "    max_df=0.5\n",
    ")\n",
    "X_count = vectorizer.fit_transform(tweets)\n",
    "vocab = vectorizer.vocabulary_\n",
    "idx_to_vocab = {idx: wd for (wd, idx) in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency experiments\n",
    "\n",
    "We use a new kind of classifier better suited to frequency information: a Multinomial Naive Bayes classifier. We use `fit_prior=False` to prevent the classifier from just learning the *prior* over the probability distribution, that class 1 is much more probable than the other two classes.\n",
    "\n",
    "We can use the weights this classifier learns to find out which specific features are most indicative of each class. We do this finding the words corresponding to the highest coefficients in the model (in the `coef_` attribute of the model) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 85.98%\n",
      "\n",
      "Recall by class:\n",
      "  0 - 49.51%\n",
      "  1 - 88.66%\n",
      "  2 - 86.16%\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(fit_prior=False)\n",
    "y_pred = cross_val_predict(model, X_count, y, cv=10)\n",
    "show_classifier_results(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for each class:\n",
      "  hate - g$t r$t#rd * h&$ tr#sh #ss wh%t$ l%k$ f#g n%gg$r n%gg# f*ck b%tch rt f#gg&t\n",
      "  offensive - g& kn&w * l&l g&t sh%t #ss g$t n%gg# f*ck l%k$ p*ss% h&$ rt b%tch\n",
      "  other - &n$ gh$tt& br&wn% m&nk$! m#k$ #mp l&l g$t !$ll&w ch#rl% !#nk$ l%k$ b%rd tr#sh rt\n"
     ]
    }
   ],
   "source": [
    "feature_model = MultinomialNB(fit_prior=False)\n",
    "feature_model.fit(X_count, y)\n",
    "n_top_entries = 15\n",
    "class_labels = ['hate', 'offensive', 'other']\n",
    "print('Top features for each class:')\n",
    "for i, class_label in enumerate(class_labels):\n",
    "        top_by_coeff = np.argsort(feature_model.coef_[i])[-n_top_entries:]\n",
    "        print(\"  {} - {}\".format(\n",
    "                class_label,\n",
    "                \" \".join(substitute_char_list([idx_to_vocab[j] for j in top_by_coeff], list_orig, list_censored))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSA\n",
    "\n",
    "To avoid the problems of sparsity from raw term frequencies, we use LSA, or *latent semantic analysis*, to give us shorter vector representations of each document. These effectively summarize the information in a term frequency matrix.\n",
    "\n",
    "To more effectively train these models, we slightly modify our `TfidfVectorizer` to enable the `use_idf` feature. This multiplies all term frequency entries for a feature by the log of the inverse of the proportion of documents that feature appeared in. This means terms that are specific to a few documents will have their weight increased, while terms appearing across all documents will be effectively reduced to 0. The way LSA summarizes the vectors is affected by the magnitude of each of the weights in the matrix, so downweighting features we don't think are informative will make this work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xanda\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "tfvectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    decode_error='replace',\n",
    "    min_df=5,\n",
    "    max_df=0.5\n",
    ")\n",
    "X_tfidf = tfvectorizer.fit_transform(tweets)\n",
    "tsvd = TruncatedSVD(n_components=20)\n",
    "X_lsa = tsvd.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA experiments\n",
    "We use LSA along with our feature counts to help \"smooth out\" some of the information about how words are related that the sparse word counts might not include. In the first model, we do this with all possible count features; in the second, we only use words that were in the top 20 most indicative features for one of the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 88.23%\n",
      "\n",
      "Recall by class:\n",
      "  0 - 27.62%\n",
      "  1 - 93.59%\n",
      "  2 - 84.39%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight=\"balanced\", penalty=\"l2\", C=0.01)\n",
    "y_pred_all = cross_val_predict(model, np.hstack((X_lsa, X_count.toarray())), y, cv=10)\n",
    "show_classifier_results(y, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_feature_idxs = set()\n",
    "for i, class_label in enumerate(class_labels):\n",
    "    top_feature_idxs.update(np.argsort(feature_model.coef_[i])[-20:])  \n",
    "X_top_count = X_count[:,list(top_feature_idxs)].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 86.54%\n",
      "\n",
      "Recall by class:\n",
      "  0 - 27.76%\n",
      "  1 - 91.72%\n",
      "  2 - 82.85%\n"
     ]
    }
   ],
   "source": [
    "y_pred_top = cross_val_predict(model, np.hstack((X_lsa, X_top_count)), y, cv=10)\n",
    "show_classifier_results(y, y_pred_top)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
